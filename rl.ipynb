{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.policies import BasePolicy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from stable_baselines3.common.utils import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"smh_stocks_max.csv\")\n",
    "df = df.fillna(0)\n",
    "df[\"volume\"] = df[\"volume\"].astype(float)\n",
    "\n",
    "close = df[\"close\"].to_numpy()\n",
    "tsa = seasonal_decompose(close, model=\"additive\", period=180)\n",
    "df[\"trend\"] = tsa.trend\n",
    "df[\"seasonal\"] = tsa.seasonal\n",
    "df[\"residual\"] = tsa.resid\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['volume', 'macd_hist', \"macd\", \"signal_line\", \"%K\", \"%D\", \"ema_200\",\n",
    "       'rsi', \"roc\", 'bb_upper', 'bb_lower']]\n",
    "targets = df[[\"trend\",\"seasonal\",\"residual\"]]   \n",
    "\n",
    "train_size = int(0.8 * len(features))\n",
    "X_train_set = features.iloc[:train_size, :]\n",
    "X_test_set = features.iloc[train_size:, :]\n",
    "\n",
    "y_train_set = targets.iloc[:train_size, :]\n",
    "y_test_set = targets.iloc[train_size:, :]\n",
    "\n",
    "scaler = MinMaxScaler((0,1))\n",
    "\n",
    "X_train_set = scaler.fit_transform(X_train_set.fillna(np.nan).to_numpy())\n",
    "X_test_set = scaler.transform(X_test_set.fillna(np.nan).to_numpy())\n",
    "\n",
    "y_train_set = scaler.fit_transform(y_train_set.fillna(np.nan).to_numpy())\n",
    "y_test_set = scaler.transform(y_test_set.fillna(np.nan).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLSTMPolicy(BasePolicy):\n",
    "    def __init__(self, observation_space, action_space, lstm_hidden_size, n_layers, output_size=1, activation_fn=nn.ReLU, **kwargs):\n",
    "        super(CustomLSTMPolicy, self).__init__(observation_space, action_space, **kwargs)\n",
    "\n",
    "        self.device = get_device()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=observation_space.shape[2], hidden_size=lstm_hidden_size, num_layers=n_layers, batch_first=True)\n",
    "\n",
    "        self.actor = self.build_fc(lstm_hidden_size, output_size, activation_fn=activation_fn)\n",
    "        self.critic = self.build_fc(lstm_hidden_size, 1, activation_fn=activation_fn)\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = lstm_hidden_size\n",
    "\n",
    "    def build_fc(self, input_dim, output_dim, activation_fn):\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, 64))\n",
    "        layers.append(activation_fn())\n",
    "        layers.append(nn.Linear(64, output_dim))\n",
    "        return nn.Sequential(*layers).to(self.device)\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        out, _ = self.lstm(obs)\n",
    "        out = out[:,-1,:]\n",
    "\n",
    "        actor_out = self.actor(out)\n",
    "        critic_out = self.critic(out)\n",
    "        return actor_out.unsqueeze(0), critic_out\n",
    "\n",
    "    def evaluate_actions(self, obs, hidden_state, actions):\n",
    "        lstm_out, hidden_state = self.lstm(obs, hidden_state)\n",
    "        action_probs = self.actor(lstm_out[:,-1,:])\n",
    "        values = self.critic(lstm_out[:,-1,:])\n",
    "\n",
    "        log_probs = torch.log(action_probs.gather(1, actions.unsqueeze(1)).squeeze(1))\n",
    "\n",
    "        return values, log_probs, hidden_state\n",
    "    \n",
    "    def reset_hidden_state(self):\n",
    "        return (torch.zeros(self.n_layers,1,self.hidden_size),torch.zeros(self.n_layers,1,self.hidden_size))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 11)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self, data, targets, seq_length, n_features, policy, scaler):\n",
    "        super().__init__()\n",
    "\n",
    "        self.action_space = Box(low=0, high=np.inf, shape=(1,1,3))\n",
    "        self.observation_space = Box(low=0, high=1, shape=(1,seq_length,n_features), dtype=np.float32)\n",
    "\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.scaler = scaler\n",
    "        self.policy = CustomLSTMPolicy(self.observation_space, self.action_space, 128, 3, 3, nn.ReLU)\n",
    "        self.current_step = self.seq_length\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.model = policy\n",
    "\n",
    "        self.observation = data[self.current_step - seq_length: self.current_step]\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            self.done = True\n",
    "            return self.observation, 0, self.done, {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.FloatTensor(self.observation)\n",
    "            actor_out, critic_out = self.model.forward(self.observation)\n",
    "            scaled_output = self.scaler.inverse_transform(actor_out)\n",
    "            predicted_price = scaled_output[0] + scaled_output[1] + scaled_output[2]\n",
    "\n",
    "        actual_price = self.targets[self.current_step]\n",
    "\n",
    "        reward = nn.MSELoss()\n",
    "\n",
    "        self.current_step += 1\n",
    "        self.observation = self.data[self.current_step - self.seq_length: self.current_step]\n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = self.seq_length\n",
    "        observation = self.data[self.current_step - self.seq_length: self.current_step]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
